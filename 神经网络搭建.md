SCTP神经网络——version1:
线下成绩  =========> 0.88553, 线上成绩  =========> 0.890(小数点后未知)

注: 加入学习率衰减后此模型线下分数有提升，还可参考论文做学习率循环

```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(64,kernel_size=(3,3),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    x = Dropout(rate=0.1)(X)
    X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(32, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
```

SCTP神经网络——version2:
线下成绩  =========> 可能高于以上, 线上成绩  =========> 未知

```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(64,kernel_size=(3,3),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    x = Dropout(rate=0.1)(X)
    X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(32, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
```


SCTP神经网络——version3:
线下成绩  =========> 0.88767 线上成绩  =========> 未知
看上去这个数据不适合过深的网络
```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(64,kernel_size=(3,3),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(32, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
 ```

SCTP神经网络——version3:
线下成绩  =========> 0.88862 线上成绩  =========> 未知
调整dropout成绩上升

```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(64,kernel_size=(3,3),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.15)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(32, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
```
SCTP神经网络——version4:
线下成绩  =========> 0.88871 线上成绩  =========> 未知
继续调整dropout成绩上升

```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(64,kernel_size=(3,3),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.2)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(32, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
```

SCTP神经网络——version5:
线下成绩  =========> 0.89026 线上成绩  =========> 未知
继续调整dropout成绩上升

```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(32,kernel_size=(3,3),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.2)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(32, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
```

SCTP神经网络——version5:
线下成绩  =========> 0.88942 线上成绩  =========> 未知
继续调整dropout成绩上升

```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(32,kernel_size=(3,3),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(32,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    #X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = Dropout(rate=0.1)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(32, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
```

SCTP神经网络——version6:
线下成绩  =========> 0.88924 线上成绩  =========> 未知
四层卷积得到最高分

```
def Convnet(input_shape = (10,10,2),classes = 1):

    X_input = Input(input_shape)

    # Stage 1
    X = Conv2D(16,kernel_size=(2,2),strides=(1,1),name="conv1",kernel_initializer=glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization()(X)
    X = Activation("relu")(X)
    X = Dropout(rate=0.2)(X)
    # Stage 2
    X = Conv2D(32,kernel_size=(2,2),strides=(1,1),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    X = Conv2D(32,kernel_size=(2,2),strides=(1,1),name="conv3",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.1)(X)
    X = Conv2D(64,kernel_size=(2,2),strides=(1,1),name="conv4",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    X = Activation("tanh")(X)
    X = Dropout(rate=0.2)(X)
    
    # Stage 3
    #X = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv5",kernel_initializer=glorot_uniform(seed=0))(X)
    #X = BatchNormalization()(X)
    #X = Activation("tanh")(X)
    #X = AveragePooling2D((2,2),strides=(1,1))(X)
    """
    X1 = Conv2D(64,kernel_size=(2,2),strides=(2,2),name="conv2",kernel_initializer=glorot_uniform(seed=0))(X_input)
    X1 = BatchNormalization()(X1)
    X1 = Activation("tanh")(X1)
    X1 = MaxPooling2D((2,2),strides=(1,1))(X1)
    """
    #X = concatenate([X,X1],axis=3)
    X = Flatten()(X)
    
    #X = Dense(16, activation='tanh')(X)
    X = Dense(classes, activation='sigmoid')(X)


    model = Model(inputs=X_input,outputs=X)
 
    return model
```


SCTP神经网络——version7:
线下成绩  =========> 0.8914 线上成绩  =========> 0.893
不同卷积层提取不同区域的特征得到较好结果(目前最好架构)

```
def Convnet(input_shape=(200,1,1),classes=1):
    X_input = Input(input_shape)

    X = Conv2D(32,(3,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input)
    #X = BatchNormalization()(X)
    X = Dropout(rate=0.2)(X)
    X3 = Conv2D(32,(5,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input)
    #X3 = BatchNormalization()(X3)
    X3 = Dropout(rate=0.2)(X3)
        
    X1 = Conv2D(32,(7,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input)
    #X1 = BatchNormalization()(X1)
    X1 = Dropout(rate=0.2)(X1)
    X = concatenate([X,X1,X3],axis=3)    
    
    X = Flatten()(X)  
    X = Dense(1, activation='sigmoid', kernel_initializer=glorot_uniform(seed=921212))(X) 
    model = Model(inputs=X_input,outputs=X,name='Convnet') 
    return model 
```
